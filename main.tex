\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{fontspec}
\setmainfont{Arial}
\usepackage{a4wide}
%\usepackage{myn}
\usepackage{mynamed}
%\newcommand{\instruction}[1]{\emph{#1}}
\newcommand{\instruction}[1]{}
%\newcommand{\people}[1]{\textsc{#1}}
\newcommand{\people}[1]{}
\newcommand{\comment}[1]{{\footnotesize\textsc{#1}}}
\parindent = 0pt
\setlength{\parskip}{0.9\baselineskip}
% Needed for colour at all (xcolor), and background colour in table cells
% (colortbl)

\usepackage{xcolor,colortbl}

% A package which allows simple repetition counts, and some useful commands

\usepackage{forloop}
\newcounter{loopcntr}
\newcommand{\rpt}[2][1]{%
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{#2}%
}
\newcommand{\on}[1][1]{
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{&\cellcolor{gray}}
}
\newcommand{\off}[1][1]{
  \forloop{loopcntr}{0}{\value{loopcntr}<#1}{&}
}

\title{%VR NT application\\
Computational Semantics for Degree Modifiers\\ %in the perceptual domain\\
(CoSMo)
}
\date{}
\author{}
\begin{document}
%\vspace{-10ex}
\maketitle
%\vspace{-12ex}
\section{Purpose and aims}
\instruction{State the overall purpose and specific goals of the research project.}


% \begin{itemize}
%     \item 
% Empirical question 1: is the meaning of very dependent on the predicate (``very tall" vs ``very small")

% \item Empirical question 2: given a predicate, is the meaning of very dependent on the comparison class (the type of the argument)? (``very tall building" vs ``very tall human")

% \item Empirical question 3: can the meaning of ``very" be modelled as a function from degree to degree (regardless of whether its meaning depends on predicate and/or comparison class) (and how powerful does thi, or do we need to generalise to a function from classifier to classifier (and if so, what is the required power of the transformations of the classifiers? )

% \item What about non-vague degree predicates (if there are any?) "very open
% \end{itemize}

% auxiliary questions

% What is vagueness in a probabilistic account? probability in a community? but not good for non-vague predicates.

% what about antonyms d("sort of tall") + d("sort of short")=d(tall) 

% What is a classifier?

% A classifier is a function from perceptual domain to a degree:
% \[Classifier = PerceptualDomain \to [0,1] \]

% Assume for example that we have a perceptual input represented by a vector $x$; then ``tall" could be implemented as a perceptron represented by a vector $p$, and $p \cdot x$ is the result (TODO: mapping in [0,1]).

% What is a degree modifier in general?

% more formulas and diagrams!

% Can the degree modifier be of a simple form?

% model 1: there is a function from "tall X" to "very tall X" independent of X

% So we're then looking for a function g such that
% very (tall,x) = g(tall(x))

% \[DegreeModifier = Degree \to Degree \]

%  $g$ is a function from degree to degree.  This seems quite limited.

% if not, model 2: there is a function from "tall" to "very tall"

% \[DegreeModifier = Classifier \to Classifier \]


% if you take seriously the idea  that meanings are  classifiers, then you have to model degree modifiers as functions from classifiers to classifiers.

% In general this can have very many parameters. But for the perceptron case we can let this transformation be a simple linear operator.

% \[ very (p,x)=(\Theta p) \cdot x \]

% where $\Theta$ is a matrix.

% Explainability can be achieved by modeling ... using explicit models

% Computational work does not take into account work in semantics

% Hybrid approaches, Erk paper, Oliver Lemon

% \textbf{More on models of vagueness (Fernandez and Larsson), Shalom, ProbTTR}

% Concept combination in general, this is a test case, interesting because it is not compositional in the "normal" way

% If we treat modifiers as other words, we may get strange results
% they cannot be modeled as distributions, but as modifications of distributions

% this hypothesis can be implemented in many ways: simple mathematical formulas, or as deep neural networkd

% we want transfer learning, so that if model learns "very green", it can generalise to "very tall"

% write about compositionality in neural networks in general (Marco Baroni, CLASP presentation)

% general problem: compositionality for models combining language and perception; ML does not handle compositoinality well; linguistic theories do not interface with perception; the small number of existing computational models of compositionality for perceptual language have a limited view of compositionality and do not deal with modifiers

% even if "tall" is context sensitive, there may be a non-context-sensitive meaning of "very"

% how to motivate from a explanability angle - if diagnosis system says "very high risk of..." or "fairly high risk..." we may want to be able to get a detailed explanation of what "very high risk" means in this case, including why the modifier "very" was used as opposed to "fairly". Not ok if system cannot proviude any explanation.

%From a practical point of view, 
Recent years have seen %, there has  been 
an increased interest in connecting language to perception, fuelled by progress in image recognition, image captioning and Visual Question Answering using deep learning methods (see e.g. \citecomma{antol2015vqa,you2016image,monroe2016learning}). Many models for doing this rely explicitly or implicitly on the idea of modeling meanings of perceptual words (i.e. words describing perceivable properties of objects and situations) as \emph{classifiers}, i.e. functions that take perceptual input (e.g. from a camera) and linguistic input (some natural language phrase or sentence) and decide to what degree the phrase describes the perceptual input. %Alternatively, a classifier can  take perceptual input and produce a natural language description (image captioning). 
%A classifier is a function from perceptual domain to a degree:

\[Classifier = PerceptualDomain \to [0,1] \]

If the possible degrees are limited to 0 and 1, we have a binary classifier; if it can be any real number between 0 and 1, we have a continuous (e.g. probabilistic) classifier. Following \cite{schlangen16}, we refer to this as the \emph{words-as-classifiers} approach. 


%The connection between language and perception has a long history in philosophy and linguistics, and is currently a hot topic in Natural Language Processsing, and much work has been devoted to models connecting natural language sentences to images in the context of Image Captioning, Visual Question Answering and Visual Dialogue.



One important aspect of language in general is \emph{compositionality} -- how the meaning of a sentence can be computed from the meanings of its parts. In  machine learning,  explicit modelling of language  is shunned in preference of neural or other  statistical models, and it is hoped that issues related to compositionality will be automatically resolved. However, many researchers take issue with this assumption and argue that the best way forward is a hybrid approach combining explicit (formal) modelling with machine learning. 

%RQ1: Are current machine learning models able to learn the meaning of words like "very"?

Existing models of compositionality for perception-related aspects of linguistic meaning, however, often take a simplistic view of compositionality and assume, essentially, that the meaning of the different parts of a sentence can %simply be \emph{added together}. That is, the meanings of the component words can 
simply be combined by conjunction (or intersection). This may work well in many cases. For example, it seems reasonable to say that a happy tall man is someone who is happy, tall, and a man. Or in the continuous case, the degree to which someone $x$ is a %happy
tall man can be given by some function $f$ from the degrees to which $x$ is %happy, 
tall and a man (which we assume below are given by  classifiers %$d_{happy}$, 
$d_{tall}$ and $d_{man}$:

%\[d_{\textrm{happy tall man}}(x) = f(d_\textrm{happy}}(x), d_\textrm{tall}(x), d_\textrm{man}(x)))\]

\[d_{\textrm{tall man}}(x) = f( d_\textrm{tall}(x), d_\textrm{man}(x)))\]

\comment{need to mention context effect of "man" on "tall" here, just to show we know.}

We refer to this property as \emph{intersective compositionality}. In general, for any phrase, %if there is a function 
%\item Definition, discrete case: $c$ is obtained by intersective composition from $c_1$ and $c_2$ provided that $s:c$ iff $s:c_1$ and $s:c_2$
%%  \begin{itemize}
%  \item $s$:\mng{light green} iff $s$:\mng{light} and $s$:\mng{green}
%  \end{itemize}
%\item Definition, generalisation: 
the degree to which some $x$ is $w_1 w_2$ (for example, $w_1$=very, $w_2$= tall) is obtained by intersective composition from $w_1$ and $w_2$ just in case there is an $f$ such that \[\delta_{w_1 w_2}(x) = f(\delta_{w_1}(x), \delta_{w_2}(x))\]

where $\delta_w(x)$ is the degree to which $x$ is judged to be $w$.


Some common constructions, however, clearly do not have this property. Computing the meaning of a phrase like ``a very tall man" is different in an important respect from computing the meaning of something like ``a happy tall man".  A very tall man is not someone who is tall, man, and ``very'' - this does not even make sense.

\[d_{\textrm{very tall man}}(x) \neq f(d_\textrm{very}}(x), d_\textrm{tall}(x), d_\textrm{man}(x)))\]

Instead, ``very'' combines with an adjective like ``tall'' by modifying it into a new predicate "very tall", so that a very tall man is someone who is a man and very tall. We refer to this as non-intersective compositionality.

\[d_{\textrm{very tall man}}(x) = f(d_\textrm{very tall}}(x),  d_\textrm{man}(x)))\]

Words like ``very” and ``sort of” are \emph{degree modifiers}
and  can be used to modify degree adjectives such as ``tall",``blue" or ``nice". But how are does this work, exactly? The short answer is: we do not know.

\comment{later, we provide some possible answers}


%Degree modifiers are words like ``very” and ``sort of”  that can be used to modify degree adjectives such as ``tall",``blue" or ``nice". % Computing the meaning of a phrase like ``a very tall man" is different in an important respect from computing the meaning of something like ``a happy tall man". In the latter case, the meanings of the component words can simply be combined by conjunction (or intersection) - a happy tall man is someone who is happy, tall, and a man. We refer to this property as intersective compositionality. By contrast, a very tall man is not someone who is tall, man, and ``very" - this does not even make sense. Instead, ``very" combines with an adjective like ``tall" by modifying it into a new predicate "very tall", so that a very tall man is someone who is a man and very tall. We refer to this as non-intersective compositionality.

Work  in the words-as-classifiers approach has so far  not taken   non-intersective compositionality seriously, nor has it focused specifically on the problem of degree modifier semantics. Existing datasets for evaluation of image captioning systems and related systems do not include degree modifiers in any systematic manner, and it is therefore unclear whether these systems are able to learn the kind of non-intersective compositionality that degree modifiers display. %This means that existing systems may in fact not be capable of dealing appropriately with non-intersective meanings, and thus limited in the coverage they can achieve on tasks related to linguistic image classification. % In the end, this imposes serious limitations on the ability of machines to understand human language and to relate linguistic utterances to perceptual information from the world.

From a theoretical point of view, the words-as-classifiers approach could form the basis of a formal semantics addressing the connection between language and perception, i.e., the symbol grounding problem (\citecomma{Harnad1990}; \citecomma{Larsson2015formal}). This, however, requires taking seriously the problem of compositionality for perceptual meanings: given that perceptual meanings of individual words are modelled as classifiers, how are these classifiers combined to reflect the meaning of a full sentence? To deal with degree modifiers in this context requires developing an account of non-intersective compositionality for perceptual meanings in general, a move which will significantly extend the applicability of the idea of modeling perceptual meanings using  classifiers.

The aim of this project is to give a hybrid computational account of the compositional semantics of degree modifiers in the domain of perceptual adjectives. This project is part of a larger research program aiming to provide a compositional formal semantics connecting language and perception.% account of the gemeaning of degree modifiers when applied in the visual domain (and possibly also other domains).

 The account will be ``hybrid" in the sense of combining %state-of-the-art
statistical/machine learning-based classifiers (for modeling perceptual meanings of adjectives, nouns, and other words displaying intersective compositionality) with a transparent, learnable and compositional 
formal semantics.
\vspace{-2ex}
\section{State-of-the-art}
\vspace{-2ex}
\instruction{Summarise briefly the current research frontier within the field/area covered by the project. State key references.}

%\subsection{Words as classifiers}

Several approaches have been proposed for modelling meanings of (some) words as classifiers of perceptual (often visual) data (e.g. \citecomma{schlangen16,monroe2016learning,mcmahan2015bayesian}). This work includes modeling of degree predicates such as colour terms. Various types of classifiers have been used, and the output of the classifiers has been rendered and connected to language in various ways (see \cite{larsson2017compositionality}\/ for an overview). This approach has also been used in work on image visual question answering (\citecomma{andreas2016learning}). 

%\subsection{Degree modifiers, gradeable adjectives and vagueness}

Degree modifiers can be applied to adjectives (``John is very tall") or adverbs (``John runs very quickly"). Crucially, the predicate being modified (e.g. ``tall" and ``quickly") must be a matter of degree. Based on this assumption, it does not make sense to say `156 is a very even number", since the property of being an even number is not a matter of degree. Predicates of degrees are often studied in terms of vagueness, and thus different accounts of vague predicates in natural language are relevant to the present project. 



%Bayesian modeling of vagueness and other NL phenomena; Goodman, Lassiter


% \subsection{Modeling degree modifiers}


There are two main lines of analysis in formal semantics as regards degree modifiers. This is in connection to the treatment of gradable adjectives and are roughly the following:
\vspace{-2ex}
\begin{itemize}
\item \textbf{the vague predicate analysis} (e.g. \citecomma{klein:1980}). Gradable adjectives under this approach are inherently vague and so are the degree modifiers.``x is a tall man" according to \cite{klein:1980}\/ means that x is tall with respect to the set of men (the comparison set), and ``x is a very tall man" means that x is tall compared to the set of tall men. 
\item \textbf{the degree based approach}. In this analysis, degree modifiers, similarly to the positive form of gradable adjectives, involve a degree parameter that has to be higher (for positive adjectives) than a contextually provided degree parameter (e.g. a threshold that above which someone/something is considered \textit{tall}). Degree modifiers like \emph{very} are doing the exact same thing, the only difference being that the contextual parameter is exceeded by a large degree. Such an approach can be found in \cite{kennedy:2005}. 
\end{itemize}
\vspace{-2ex}
Recently,  approaches using some version of probabilistic semantics, most importantly work by Lassiter and colleagues have been put forth as regards gradable adjectives. However, there is no explicit work on degree modifiers in these approaches, so it is not clear what the account of degree modification will be in these cases. For an overview of gradable modification that might give insights on how these approaches will treat degree modifiers see \cite{morzy:2015}\/ and \cite{lassiter:2014}.

As regards the experimental testing of the above approaches using empirical work, there is very little work. There is some empirical/experimental work on gradable adjectives, but to our knowledge only one dealing with degree modifiers, that of \cite{Mcnabb2012}. The results of this study suggest that the gradable approach is more plausible to the vague predicate approach, but no other alternatives are considered. Even though the scale of the study is relatively small (30 participants), the experimental design can give important insights for future experimental settings. 

The probabilistic approach to degree modifiers by Lassiter and others bears some important similarities to the fuzzy sets approach (\citecomma{zadeh1996fuzzy,hersh1976fuzzy}). Although the overall approach is criticized, e.g. by \cite{klein:1980}, the fuzzy sets approach to vague predicates and degree modifiers has some important advantages in the present context. Firstly, insofar as the output of a classifier can be modeled as a fuzzy set,  degree modifiers can be modeled as operations on fuzzy sets. In our approach, this translates to modeling degree modifiers as functions from classifiers to classifiers. Secondly, work in the fuzzy sets approach provides one of the few empirically validated models of degree modifiers, and offers a blueprint for the empirical approach we will take in this project.

%The criticism against the fuzzy sets approach to semantics is mostly directed against the idea that fuzzy truth values of sentences are derived by composition of the truth values of the components of the utterance. We agree with this criticism and 

While we  base  our analysis of degree modifiers on work in the fuzzy sets tradition, we will avoid the problems connected to this approach. Instead we want to combine our analysis of degree modifiers with probabilistic semantics of the kind advocated by \cite{goodman2015probabilistic}\/, but cast in a probabilistic type theoretical framework (\citecomma{cooper2012type,cooper:ttnls,cooper2017adapting}).

There are also other models of vagueness that we want to investigate with respect to their potential use in further improving our analysis  and formalisation, in particular concerning how to relate degrees of truth with probability. These include subjective logic (\citecomma{josang1997artificial}) and  rough sets (\citecomma{Pawlak1982,Agarwal2016}).

%- degree of confidence of speaker (epistemic prob)  + degree of applicability (in simple model, define applic in terms of epistemic probality)




% They also admit that something like TTR may have some advantages here, which supports our choice of using probabilistic TTR.

% \begin{itemize}
% \item We have not been able to find any account of non-intersective compositionality on the opaque functions approach \textbf{[NON-]}.
% \item However, we have found something even better - an account of non-intersective compositionality which is independent of classifier type.
% \item Not originally formulated in terms of classifiers, but in the theory of fuzzy sets\\ 
% \item We are adapting this to probabilistic TTR
% %\item The only assumption is that the classifier outputs a number (e.g. a probability, i.e. a number between 0 and 1)
% \item Joint work with Jean-Philippe Bernardy and others
% \end{itemize}





% Degree modifiers is a  fairly well-studied area in semantics, including formal semantics. However, while several formal models have been proposed, there has been little empirical work in testing and comparing the coverage of these hypotheses.



%\comment{SC: Something that might be useful in terms of terminology, this is from Rett's 2008 "Degree Modification in Natural Language" thesis: \textit{To my knowledge, the first use of this sense of the term ‘degree modifiers’ is in Paradis (1997). Paradis uses the term to subsume several different categories to which modifiers of adjectives and verbs had previously been assigned} }





% \textbf{Mehdi:}

% Yann LeCun recently had this post about importance of "Differentiable Programming”:
% \url{https://www.facebook.com/yann.lecun/posts/10155003011462143}

% A paper under review on CLEVR dataset using this term "fully differentiable non-modular networks”:
% \url{https://openreview.net/pdf?id=S1Euwz-Rb}

% (without reading the paper just based on the second author)

% % Different topic: a short paper on cardinals and quantifiers for vision might have some related works: 
% % \url{http://aclweb.org/anthology/E17-2054}

% \textbf{Louise McNally:}

% Louis McNally: There has been work, but it's been more on a) trying to determine which modifiers go with which adjectives, b) on the adjectives themselves and how their standards are identified, or c) other kinds of degree modifiers, particularly things like "at least/most" or comparatives. Still, you might find some things of interest in the following places:

% This paper by Yaron McNabb: \url{http://mitwpl.mit.edu/open/sub16/McNabb.pdf}

% I haven't read McNabb's work, so I don't know whether his dissertation, which was on degree modification, will contain anything of interest.

% These papers by Michael Franke:

% \url{http://www.sfs.uni-tuebingen.de/~mfranke/Papers/SchollerFranke_2017_Surprisingly%20Marker%20of%20Surpirse%20readings%20or%20Intensifier.pdf}

% \url{http://www.sfs.uni-tuebingen.de/~mfranke/Papers/AC-post_proc-2011.pdf}

% The first of these two papers replicates an earlier piece of work by Stephanie Solt and Nicole Gotzner. Stephanie has done a lot of experimental work on adjectives, on the one hand, a different kinds of degree modification, but, again, not specifically intensifiers like "very". Maybe there is something in the "tolerant/classical/strict" literature on vagueness, which takes a non-degree-based approach to adjective meaning (Rob van Rooij, Paul Egré, Heather Burnett), but I haven't checked that out.


\section{Significance and scientific novelty}\instruction{Describe briefly how the project relates to previous research within the area, and its importance in the short and long term. Describe also how the project moves forward or innovates the current research frontier.}



The approach we will explore is to model degree modifier semantics as functions from perceptual classifiers (e.g. for ``big”) to perceptual classifiers (``very big”). While this idea is related to and builds on previous work in several research areas, we believe that it is novel and offers promising new perspectives not only on degree modifier semantics, but also on the connection between linguistic meaning and perceptual classification, the formalisation of non-intersective compositionality, and in general the compositional nature of perceptual meaning. %\textbf{, and machine learning approaches to visual question answering and image captioning}.


This approach has the added scientific advantage of allowing testing of a couple of novel hypotheses. \cite{hersh1976fuzzy}\/ have shown that degree modifiers like ``very" can be modeled as nonlinear functions and that such functions can be empirically tested, compared and verified. However, we have not found any work where such functions are tested across several different predicates from different perceptual domains, to investigate e.g. whether ``very" means the same when used in the phrase "very large" as when used in ``very blue". Furthermore, while it is well known that the semantics of degree predicates are sensitive to the word they modify, so that ``tall" in ``a tall basketball player" means something slightly different than in ``a tall boy", we want to find out if degree modifiers display similar dependencies (we suspect that they do not).

We will carry out data collection efforts to test operationalised hypotheses concerning the meanings of degree modifiers and the extent to which these meanings are sensitive to linguistic and perceptual context. Currently, no such dataset is available for researchers. This project will help establish degree modifier semantics, and more generally non-intersective compositionality, as an empirically-based branch of semantic theory and language technology by providing a gold standard corpus that can be used for training and testing not only our approach, but also competing approaches. %\textbf{The dataset collected in the project will also be useful in the development of implemented models of Visual Question Answering and image captioning.}






\section{Preliminary and previous results}\instruction{Describe briefly your own previous research and pilot studies within the research area that make it probable that the project will be feasible. State also if no preliminary results exist. State whether the project contributes further to research and scientific results from a grant awarded previously by the Swedish Research Council.}

In \cite{Larsson2015formal}, a formal and compositional semantics for low-level perceptual aspects of meaning is presented, tying these together with the logical-inferential aspects of meaning traditionally studied in  formal semantics. The key idea is to model perceptual meanings as classifiers of perceptual input. We use Type Theory with Records (TTR), a formal semantics framework which starts from the idea that information and meaning is founded on our ability to perceive and classify the world, i.e., to perceive objects and situations as being of types. The problem of non-intersective compositionality is briefly discussed.  


Larsson (\citeyear{larsson2017compositionality}\/) compares three approaches to compositional semantics for words that are modeled (in part) using perceptual classifiers: ``meanings as sets", ``meanings as transparent functions", and ``meanings as opaque functions".  The approaches are evaluated according to whether they fulfill a set of desiderata, including dealing with non-intersective compositionality, working with state of the art classifiers, and accounting for learning of perceptual meanings.  We find that none of the current approaches fulfill all our desiderata.

The approach advocated in the present project seeks to fix exactly this problem: a compositional perceptual semantics which handles both intersective and non-intersective compositionality, deals with vagueness, connects to work in formal semantics, and is compatible with state-of-the-art visual classifiers.



\section{Project description}
\instruction{Describe the project design, including the following items}

\subsection{Theory and method}
\instruction{Describe the underlying theory and the methods to be applied in order to reach the project goal.}

% Theory:
% \begin{itemize}
% \item TTR
% \item perceptual meanings as classifiers
% \item ...?
% \end{itemize}

%We will build on the "words as classifiers" approach to modeling the meanings of degree predicates. 





The project will employ several methods, each carried out within a separate workpackages, each feeding into the other:  Data collection and hypothesis testing (WP1), Formalisation (WP2) and  Computational implementation  (WP3).





\subsubsection*{WP1. Data collection and hypothesis testing} 

\people{chris, staffan, (stergios), simon?}

This WP contains the main empirical work in the project. The goal is to find out how to best model the meanings of degree modifiers in a words-as-classifiers approach to perceptual semantics. This will be done by collecting human judgements against which a number of hypotheses are tested. The judgements will be collected using crowdsourcing using Amazon Mechanical Turk, and the language used will be English. The visual domain has the advantage that it is relatively easy to get empirical data where each datapoint has explicit values for the relevant conceptual  dimensions (e.g. height for ``tall").

Hersh et al. \citeyear{hersh1976fuzzy}\/  investigate the adjective ``large" when applied to the noun ``box". We will build on and generalise this approach by collecting human judgements about the appropriateness of various phrases for describing images which vary along the relevant dimensions (e.g. size, colour, shape). 
The basic idea is to compare the degree to which the members of a linguistic community (e.g. Swedish speakers or English speakers) are willing to accept a verbal description as an appropriate description of a visual stimuli (a picture). This will be done in two ways: categorical judgements (yes or no) and graded judgements (e.g. a number between 1 and 10). We can then compare aggregated judgements across speakers as well as individual judgements, yielding for each pair of picture and linguistic  stimuli a number.

By asking subjects to judge, for each stimulus, the appropriateness of a description with (``a very large box", ``a very blue box") and without (``a large box", ``a blue box") a degree modifier, we can compute the relative contribution of the degree modifier. (The computation of the relative contribution of the degree modifier can be seen as analogous to learning (part of) the meaning of the degree modifier by an agent.) 

We will look at judgments concerning at least three different degree predicates referring to different perceptual domain, e.g. ``tall" (referring to height), ``blue" (referring to colour") and ``large" (referring to size). The exact set of predicates will be determined early on in the project on the basis  of a small pilot study.


%\comment{which languages?}

%\comment{"tall" is context dependent, but "very" might not be - argue in detail!}

%\comment{do we want to model context dependence? similar questions also apply to this, e.g. is there a general meaning of "tall" (no) , is there a mapping across domains for different meanings of tall (maybe), or does it have to be learnt specifically for each category (man, building, etc.)? or try to avoid context dependence in the dataset? no other large/small things than boxes, no other blue/red things than blobs.}

An important fact about degree predicates is that they are sensitive to the type of entity they are predicated of. For example, ``tall" in the context ``a tall basketball player" is associated with a greater height than in the context ``a tall boy". We believe that while degree predicates are indeed context sensitive in this way, degree modifiers are not, so that ``very" in "a very tall basketball player" has the same meaning as in ``a very tall boy". We will test this hypothesis against our data. If confirmed, it will allow us to simplify later data collection by avoiding effects of context dependence in the dataset, by using stimuli with only one type of entity per degree predicate (e.g. no other large things than boxes, no other blue things than blobs).

%We hypothesize that "very" modifies "tall" in the same way that it modifies "blue" -- that is, given a degree $d$ of tallness or blueness, the model (function $f$) of "very" will give the same value for "very tall" and "very blue", namely $f(d)$. 

We will compare judgements about sentences involving degree modifiers applied to adjectives from different domains, e.g. size (``the box is very large"), colour (``the box is very blue") and shape (``the nail is very bent"), with judgements concerning the same expressions but excluding the degree modifier (``the box is large", etc.). This will allow us to compare the relative contribution of the degree modifiers to the appropriateness of the various descriptions, and to investigate  hypotheses such as the following (ranked by decreasing generality):


%that (1) the meanings of such modifiers are  domain-independent, enabling cross-domain learning, and (2) these meanings  can be modeled as non-linear (but not as linear) functions. 

%hypothesis 1:
\begin{itemize}
\item[H1] each degree modifier $m$ corresponds to a function $f_m$ which is identical across degree domains (height, colour etc.) and entities (boxes, buildings etc.)%, so that e.g. $f_{very}(large
\item[H2] each degree modifier $m$ corresponds to a  different function $f_m^d$ depending on the  domain  $d$, but there is a mapping $g$ between functions which is the same for all degree modifiers so that $g(f_m^d)=g(f_m^e)$ for domains $d$ and $e$ 
\item[H3] each degree modifier $m$ corresponds to a  different function depending on the  domain and entity %, but there is a domain-dependent mapping between the functions
\end{itemize}

%In Year 1, in addition to formulating testable hypotheses such as H1-H3 above, we will create an experimental design which minimizes unrelated effects, create suitable materials for data collection (image-text pairs), and operationalize the hypotheses with respect to the experimental design. 

%In Years 1 and 2, we will carry out data collection using Amazon Mechanical Turk and test hypotheses against collected data. A smaller pilot study will be carried out to test the experimental setup, before starting the main data collection effort. 

The pilot study is expected to take 6 months in total for preparation, data collection and analysis. An initial deliverable will be a conference publication detailing the pilot study (Year 1, Q4).

Preparing the main data collection effort, including creating suitable materials for data collection (image-text pairs),  is expected to take 4 months. Data is expected to take 4 months to collect and up to 4 months to analyse (in parallel with the other strands of research). The main deliverable of WP1 (Year 2, Q4) will be a journal article presenting the data collection effort, the testing of our hypotheses, and the results. The data will be made freely available. %, and could  a test suite of probabilistic inferences relating to gradability and vagueness.

%SC: Perhaps say that we will also construct a test suite to evaluate similar systems at the end, containing relevant examples? Just a thought 

%The rest of the work in this WP will be spent writing up the results, which will be presented in a journal article (Year 2, Q4). 

The experiments will be designed and carried out by Howes (lead) and Larsson, with assistance from Chatzikyriakidis and Dobnik.

% We will:

% \begin{itemize}
% \item  Formulate testable hypotheses such as H1-H3 above. [Y1]
% \item Create an experimental design which minimizes unrelated effects. [Y1] 
% \item Create suitable materials for data collection (image-text pairs). [Y1]
% \item Operationalize the hypotheses with respect to the experimental design. [Y1]
% \item Carry out data collection using Amazon Mechanical Turk. [Y1-Y2]
% \item Test hypotheses against collected data. [Y1-Y2]
% \end{itemize}

% A smaller pilot study will be carried out to test the experimental setup [Y1], before starting the main data collection effort [Y1]. 

%The collected dataset could possibly also be used for machine learning and a variety of other models; we will offer this as a resource to the community.

\subsubsection*{WP2. Formalisation}
\people{rasmus, staffan, robin, stergios, jean-philippe?}

The goal of this WP is to provide a formal model of perceptual semantics, in particular focusing on degree modifiers, and encompassing the empirical findings from WP1. 


We will build on the ``words as classifiers" approach to modeling the meanings of perceptual predicates
(\citecomma{Larsson2015formal,fernandezvagueness,schlangen16,larsson2017compositionality}), and extend it to the modeling of degree modifiers.

As described in \cite{Larsson2015formal}, Type Theory with Records (TTR) offers a framework where low-level perceptual aspects of meaning are tied together with  logical-inferential aspects of meaning traditionally studied in  formal semantics. The key idea is to model perceptual meanings as classifiers of perceptual input. TTR starts from the idea that information and meaning is founded on our ability to perceive and classify the world, i.e., to perceive objects and situations as being of types. 

A classifier can be seems as a distribution of degrees (or probabilities) over a conceptual or perceptual space. One approach to modeling the semantics of non-intersective degree modifiers in a domain-independent way is to see them as non-linear functions, taking an input value delivered by a classifier, and outputting a modified value.  We will embed this analysis in the formal framework on Probabilistic TTR (\citecomma{cooper:ttnls,fernandezvagueness}).

Regardless of what type of classifier is used to model perceptual meanings of degree predicates, it appears that they can be modeled as functions from $n>0$ dimensions of variation to a degree. For example, tallness can be modeled as a function taking a height (a single dimension) and yielding a degree of tallness (e.g. a real number between 0 and 1). A colour term such as ``blue" can be seen as a function from a 3-dimensional colour space (colour, hue and brightness) to degrees of blueness. This means that an account of degree modifiers that is consistent with this way of modeling degree predicates can remain agnostic to the precise way that the degree predicate is modeled. For example, we hypothesise that ``very" modifies ``tall" in the same way that it modifies "blue" -- that is, given a degree $d$ of tallness or blueness, the model (function $f$) of "very" will give the same value for ``very tall" and ``very blue", namely $f(d)$. This allows us to embed classifiers in in Probabilistic TTR as abstract functions, which gives flexibility in the exact way these classifiers are computationally implemented (in WP3). 


 







%It appears that the model of degree modifiers we want to focus on has been overlooked in the literature, but we believe this is a case of "guilt by association". While there is legitimate arguments against fuzzy sets as a general model of natural language semantics, we believe that the kind of model of degree modifiers espoused within that framework is very promising, and can be embedded in a probabilistic framework where the criticized assumptions of fuzzy set theory can be avoided.


%Regardless of what type of classifier is used to model perceptual meanings of degree predicates, it appears that they can be modeled as functions from $n>0$ dimensions of variation to a degree. For example, tallness can be modeled as a function taking a height (a single dimension) and yielding a degree of tallness (e.g. a real number between 0 and 1). A colour term such as "blue" can be seen as a function from a 3-dimensional colour space (colour, hue and brightness) to degrees of blueness. This means that an account of degree modifiers that is consistent with this way of modeling degree predicates can remain agnostic to the precise way that the classifier corresponding to the degree predicate is internally modeled.

%In more detail, we will:

%\%begin{itemize}
%\item 


In Year 1, we will build on Probabilistic TTR to provide the fundamentals of a compositional hybrid semantics combining transparent functions modeling degree modifiers with machine-learned classifiers, %(e.g. deep learning classifiers),
thus %\textbf{(1) offering a way of enhancing the performance of current approaches to visual question answering and image captioning, and (2)} 
connecting such classifiers to formal semantics. This account will cover both intersective and non-intersective compositionality. We will also evaluate alternative approaches of vagueness with respect to the modeling of degree modifiers.%, including bayesian and non-bayesian probabilistic approaches, fuzzy sets, rough sets, subjective logic (\citecomma{josang1997artificial}\/) etc., and relate these to the  TTR formalisation. 

In Years 2 and 3, we will enrich our model by formalising the findings of WP1 in terms of the correct level of generalisation (as described e.g. by the hypotheses H1-H3) with respect to the modeling of degree modifiers. 

The main deliverable of WP2 (Q2, Year 3) will be a journal article presenting our formal model of degree modifier semantics.  The formalisation will be carried out by Larsson (lead), Bernardy,  Chatzikyriakidis and Blanck  with assistance from Cooper,  Dobnik and Howes.

%\end{itemize}


%\comment{RC: partee paper?}

\subsubsection*{WP3. Computational implementation}

\people{jean-philippe, simon?, mehdi,  Staffan, additional programmer?}

The goal of this WP is to implement the formalised models, provided by WP2.  We will implement a variant of Probabilistic TTR to be used for the implementation of our model from WP2. %SC: Do we want to say something like"a variant of Probabilistic TTR" that will be exressive enough but at the same time computationally tractable? (this connects to our short discussion on the stairs we had about FOTTR). Just a thought
In this implementation, we will provide classifiers (deep learning-based) as abstract functions, to be instantiated by off-the-shelf implementations as well as (if time permits) classifiers that we have trained from our own data.

In Year 1 (Q3-Q4), we will implement   our variant of Probabilistic TTR. In Year 2, we will encode the model developed in WP2 on the basis of results from WP1, thus extending the computational implementation of Probabilistic TTR developed in Year 1. In Year 3, we will further extend the computational implementation.

In Year 3, we will extend our implemented model to support simple visual question answering on limited domain data (including generated images). We will evaluate the implemented system on a test set involving degree modifiers (similar to the approach of \cite{andreas2016learning}).


The final deliverable of the project will be a computational implementation of our formal model, which can give graded judgements of the appropriateness of a verbal description (including degree modifiers) for describing an image, as well as a journal article presenting the implementation and evaluation (Q4, Year 4). The implementation will be carried out by Bernardy (lead), Larsson, Blanck and Dobnik  with assistance from Cooper.
%SC: What about specifically talking about a test suite of probabilistic inferences relating to gradability, vagueness etc. that we will evaluate against?

% Concretely, we will: 
% \begin{itemize}
% \item Implement a probabilistic logic (a variant of Probabilistic TTR) [Y1]
% \item In this implementation, provide the classifier (deep learning-based) as abstract functions [Y1-Y2]
% \item Encode the models of WP2 in the formalism of this Probabilistic TTR [Y2-Y3]
% \item Extend the formalisms to support visual question answering on limited domain data (possibly generated images) [Y3]
% \item Evaluate the implemented system on Visual Question Answering test set involving degree modifiers (similar to the approach of \cite{andreas2016learning}) [Y3]
% \end{itemize}


%\comment{what we are doing is a more sophisticated variant of Andreas et al (see also differentiable programming, compositionality of programs), connection to other work in formal semantics etc }

%\comment{Why the perceptual domain?
%- practically useful for VQA etc}



% \subsubsection*{WP4. Evaluation of implemented system}

% \people{mehdi?, simon, jean-philippe(staffan)} 

% * generate dataset for testing models, a la Andreas et al?

% * use collected data

% * small domain, 10.000 examples, simple sentences

% * large domain 100.000 examples needed, 10 cents each, 80.000 SEK, "a semi-large blueish blob sort of to the right of a very tall man"

% * generate sentences and pictures, elicit judgements



% * baseline from applying existing end to end ML techniques to our empirical data??? apply alrgorithms from others (e.g. andreas) to this domain

% * training classifiers on images and non-modified NL phrases ("large")

% * training classifiers on images and degree-modified NL phrases ("very large")

% * compare performance on learning domain-independent meanings for degree modifiers

% ? explore the limitations on current ML models when it comes to modeling non-intersective compositionally; can we exploit results from 1 to improve ML systems?


\subsection{Time plan and implementation}
\instruction{Describe summarily the time plan for the project during the grant period, and how the project will be implemented.}

The projected timetable for these three work packages is illustrated in the Gantt chart below. Key deliverables are conference and journal papers, see Section 5.1 above. 

\noindent\begin{tabular}{p{0.17\textwidth}*{20}{|p{0.01\textwidth}}|}
% The top line
\textbf{Gantt chart} & \multicolumn{4}{c|}{Year 1} 
           & \multicolumn{4}{c|}{Year 2} 
           & \multicolumn{4}{c|}{Year 3} 
\\
% The second line, with its five years of four quarters
\rpt[3]{& 1 & 2 & 3 & 4} \\
\hline
% using the on macro to fill in twenty cells as `on'
WP1        \on[8] \off[4] \\
\hline
WP2    \on[10] \off[2]\\
\hline
WP3    \off[2] \on[10]  \\
\hline
% using the on macro followed by the off macro
Deliverable     \off[3]\on[1]\off[3]\on[1]\off[1]\on[1]\off[1]\on[1]\\
\hline
\end{tabular}



\subsection{Project organisation}

\instruction{Clarify the contributions of yourself and any participating researchers to the implementation of the project. Describe and explain the competences and roles of the participating researchers in the project, and also any other researchers or corresponding who are important for the implementation of the project.}

This project is methodologically diverse, and combines insights and theories from several strands of of research (including formal linguistics, machine learning and experimental psycholinguistics). The rich research environment at FLoV and CLASP enables us to put together a well coordinated and highly skilled team with a diverse set of competences, well equipped to carry out the research specified in this application. (This means that more researchers than perhaps is usual will be involved.) 

Staffan Larsson  is an expert in formal semantics as related to perception and semantic coordination. Larsson will work in WP1, WP2 and WP3, will lead the formalisation effort (WP2), and will lead the overall project.

Christine Howes is an expert in empirical and formal research on dialogue, with strong psycholinguistics and experimental skills. Howes will lead the work on WP1.

Stergios Chatzikyriakidis is an expert on type-theoretical approaches to semantics, including analyses of degree modifiers. Chatzikyriakidis will work on WP1 and WP2.

Robin Cooper is an internationally renowned authority on formal semantics, and the originator of Type Theory with Records. Cooper will work on WP2.

Jean-Philippe Bernardy specialises in constructive type theory and machine learning, and is a highly experienced research engineer. Bernardy will work on WP2, and WP3, advising on model construction and lead the implementation work in WP3.

Simon Dobnik is an expert on embodied and situated semantics, and has implemented several systems connecting language and vision. Dobnik will work on WP1, WP2 and WP3.  

Rasmus Blanck has a PhD in logic and is specialised in the study of formal systems. Blanck will work on WP2 and WP3.

\subsection{Dissemination}

Scientific results will be communicated to the scientific community through international peer reviewed conferences and journals (e.g. Journal of Language Modelling, Cognitive Science, Computational Linguistics). We will also submit papers to the most prestigious conferences in the area (e.g. ACL, EACL, IWCS, *SEM, Cognitive Science Conference) with a total of 10 conference publications over the course of the project.


%8 people so far, but 6 + applicant max according to VR? okay with unnamed people in addition?

%1,5mkr = 1,2 full time

% \begin{itemize}
% \item 
% staffan (PI) 25\%, WP1,2,3,4
% \item stergios 5-10\% WP1,2?
% \item jean-philippe 15-20\% WP1,4
% \item chris 15-20\% WP2
% \item robin 5-10\% WP1
% \item rasmus 5-10\% WP1
% \item mehdi ?\% WP3
% \item simon? ?\% WP2,3?
% \end{itemize}

%\section*{OPTIONAL}\instruction{Provide the following information also. If a heading is not relevant to your application, please leave it blank.}

\section{Equipment}
%\instruction{Describe the basic equipment you and your team have at your disposal for the project.}

The CLASP project has a workstation with 4 GPU cards, suitable for deep-learning research. It also has an Amazon Mechanical Turk account, which we can use for data collection.


% \section{Need for infrastructure}\instruction{Specify the project’s need for international and national infrastructure. Specify also the need for local infrastructure, if depreciation costs for this are included in the application. Read more about research infrastructure supported by the Swedish Research Councilopens in new window.}

%\section{International and national collaboration}\instruction{Describe your own and the team’s collaboration with foreign and Swedish researchers and research teams. State whether you contribute to or refer to international collaboration in your research.}

%\section{Other applications or grants}\instruction{If you are applying for or intend to apply for other grants from the Swedish Research Council, the relationship between the projects shall be clarified. This applies also if you are receiving ongoing grants from the Swedish Research Council with grant periods that wholly or partly overlap with the grant you are now applying for. You should also justify why you are submitting one or several further applications. Describe also the relationship with other applications to or grants from other funding bodies for the same project concept (from you or another researcher).}

%Bernardy is applying for a starting grant ``Deep Learning and Functional Programming''. If both projects are granted, part of this project will be using the outputs of Bernardy's project, as infrastructure.

%Dobnik is applying for 20\% on the VR-NT 2018 proposal ``Learning language, action, and perception'' (LAP). The project is on a different topic but shares synergies with this project in terms of its methodology.

%Larsson is 

\section{Independent line of research}\instruction{If you are working or will be working in a larger group, please clarify how your project relates to the other projects in the group. If you are continuing a project that was wholly or partly started during your doctoral or postdoc studies, you must also describe the relationship between your project and the research of your former supervisor.}

% The present project, if funded, would be affiliated with CLASP (VR 2014-39), which is "devoted to research and advanced training in the application of probabilistic modelling and machine learning methods to core issues in linguistic theory and cognition." (quoting from \url{https://clasp.gu.se/about}, 2018-03-09). The present project thus fits within the overall aim of CLASP and

The project will be located within The Centre for Linguistic Theory and Studies in Probability (CLASP, led by Shalom Lappin, supported by the Swedish Research Council 2014-39) in our department at the University of Gothenburg. One of the centre’s focus areas is perception and language.


% Dobnik leads the research group on perception and language within CLASP. Howes is PI of the project  

{\scriptsize
\bibliographystyle{mynamed}

\bibliography{main}
}

\end{document}
